{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input: labeled csv file of start and end times, along with species label (which could be 'unknown')\n",
    "# output: preprocessed calls for use with the classifier\n",
    "# scroll down to 'user input begins here' to use\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "from scipy.signal import firwin, filtfilt,lfilter,butter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define supporting functions ----------------------\n",
    "def find_csv_file(directory):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            return os.path.join(directory, file)\n",
    "    return None\n",
    "\n",
    "def load_metadata(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def create_directory_structure(base_path):\n",
    "    \"\"\"\n",
    "    Create the following folder structure:\n",
    "    base_path/\n",
    "        |---segmented_audio\n",
    "            |---raw\n",
    "            |---bandpass\n",
    "            |---denoised\n",
    "    \"\"\"\n",
    "    subfolders = ['segmented_audio']\n",
    "    subsubfolders = ['raw', 'bandpass', 'denoised']\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        for subsubfolder in subsubfolders:\n",
    "            folder_path = os.path.join(base_path, subfolder, subsubfolder)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            print(f\"Created directory: {folder_path}\")\n",
    "\n",
    "def fir_bandpass_filter(data, lowcut, highcut, fs, numtaps=2048, window='hamming'):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = (lowcut) / nyquist\n",
    "    high = (highcut) / nyquist\n",
    "    b = firwin(numtaps, [low, high], pass_zero=False, window=window)\n",
    "    y = filtfilt(b, 1.0, data)\n",
    "    return y\n",
    "\n",
    "def highpass_filter(data, cutoff_freq, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    cutoff = cutoff_freq / nyquist\n",
    "    b, a = butter(order, cutoff, btype='high', analog=False, output='ba')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def noisereduce_wav(y, sr):\n",
    "    epsilon = 1e-10\n",
    "    y = nr.reduce_noise(y=y+epsilon,stationary=False, sr=sr,n_fft=2048,\n",
    "                        prop_decrease=0.99,\n",
    "                        win_length=2048,\n",
    "                        hop_length=512,\n",
    "                        time_mask_smooth_ms=50,\n",
    "                        freq_mask_smooth_hz=50,\n",
    "                        n_jobs=-1)\n",
    "    return y\n",
    "\n",
    "def normalize_amplitude(original, processed):\n",
    "    original_rms = np.sqrt(np.mean(original**2))\n",
    "    processed_rms = np.sqrt(np.mean(processed**2))\n",
    "    if processed_rms == 0:  # Avoid division by zero\n",
    "        return processed\n",
    "    gain = original_rms / processed_rms\n",
    "    return processed * gain\n",
    "\n",
    "def match_target_amplitude(y, target_dBFS=-20):\n",
    "    rms = (y ** 2).mean() ** 0.5\n",
    "    scalar = 10 ** (target_dBFS / 20) / (rms + 1e-9)\n",
    "    return y * scalar\n",
    "\n",
    "def apply_fade(audio, sr, fade_duration=0.1):\n",
    "    fade_samples = int(fade_duration * sr)\n",
    "    fade_in = np.linspace(0, 1, fade_samples)\n",
    "    fade_out = np.linspace(1, 0, fade_samples)\n",
    "\n",
    "    audio[:fade_samples] *= fade_in\n",
    "    audio[-fade_samples:] *= fade_out\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def pad_audio_with_clipping(audio, target_duration, sample_rate, original_audio, start_idx, end_idx):\n",
    "    \"\"\"\n",
    "    Pad the audio with clipping from just before and after the segment.\n",
    "\n",
    "    Args:\n",
    "    audio (np.array): The audio data to pad.\n",
    "    target_duration (float): The desired duration in seconds.\n",
    "    sample_rate (int): The sample rate of the audio.\n",
    "    original_audio (np.array): The original audio data to use for padding.\n",
    "    start_idx (int): The start index of the segment in the original audio.\n",
    "    end_idx (int): The end index of the segment in the original audio.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The padded audio data.\n",
    "    \"\"\"\n",
    "    current_samples = len(audio)\n",
    "    target_samples = int(target_duration * sample_rate)\n",
    "    \n",
    "    if current_samples < target_samples:\n",
    "        pad_length = target_samples - current_samples\n",
    "        pad_left = original_audio[max(0, start_idx - pad_length // 2):start_idx]\n",
    "        pad_right = original_audio[end_idx:min(len(original_audio), end_idx + pad_length - len(pad_left))]\n",
    "        \n",
    "        # If not enough padding on either side, use remaining padding from the other side\n",
    "        if len(pad_left) + len(pad_right) < pad_length:\n",
    "            if len(pad_left) < pad_length // 2:\n",
    "                pad_right = original_audio[end_idx:end_idx + pad_length - len(pad_left)]\n",
    "            if len(pad_right) < pad_length // 2:\n",
    "                pad_left = original_audio[start_idx - pad_length + len(pad_right):start_idx]\n",
    "        \n",
    "        padded_audio = np.concatenate([pad_left, audio, pad_right])\n",
    "        #print(f\"Padding audio from {current_samples} to {target_samples} samples\")\n",
    "        return padded_audio\n",
    "    else:\n",
    "        return audio[:target_samples]  # Truncate if longer than target duration\n",
    "\n",
    "def process_and_segment_audio(audio, sr, df_file, output_base_path, fmin, fmax, segment_length=2.0):\n",
    "    \"\"\"\n",
    "    Process and segment audio files based on metadata from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    audio (np.array): The audio data.\n",
    "    sr (int): The sample rate of the audio.\n",
    "    df_file (pd.DataFrame): DataFrame containing metadata for the audio file.\n",
    "    output_base_path (str): Base path for output directories.\n",
    "    segment_length (float): Length of each segment in seconds.\n",
    "    overlap_percent (float): Percentage of overlap between adjacent segments.\n",
    "    \"\"\"\n",
    "    #print(\"Applying band-pass filter...\")\n",
    "    # filtered_audio = highpass_filter(audio, 150, sr)\n",
    "    filtered_audio = fir_bandpass_filter(audio, fmin, fmax, sr)\n",
    "    \n",
    "    filtered_audio = match_target_amplitude(filtered_audio)\n",
    "    \n",
    "    #print(\"Applying noise reduction to entire audio...\")\n",
    "    denoised_audio = noisereduce_wav(filtered_audio, sr)\n",
    "    \n",
    "    denoised_audio = match_target_amplitude(denoised_audio)\n",
    "    \n",
    "    raw_dir = os.path.join(output_base_path, 'segmented_audio', 'raw')\n",
    "    bandpass_dir = os.path.join(output_base_path, 'segmented_audio', 'bandpass')\n",
    "    denoised_dir = os.path.join(output_base_path, 'segmented_audio', 'denoised')\n",
    "    \n",
    "    for _, row in df_file.iterrows():\n",
    "        start_time = round(row['start_time'], 2)\n",
    "        end_time = round(row['end_time'], 2)\n",
    "        species = row['species'].replace(\" \", \"_\")\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        #print(f\"Processing segment for {species} from {start_time} to {end_time}\")\n",
    "        \n",
    "        if duration <= segment_length:\n",
    "            # For segments shorter than or equal to 1.5 seconds\n",
    "            segment_starts = [\n",
    "                start_time,\n",
    "                # max(round(start_time - (segment_length - overlap_percent * segment_length), 2), 0),\n",
    "                # min(round(start_time + (segment_length - overlap_percent * segment_length), 2), len(audio) / sr - segment_length)\n",
    "            ]\n",
    "            # segment_ends = [segment_starts[0] + duration, segment_starts[1] + duration ,min(segment_starts[2] + duration, end_time)]\n",
    "            segment_ends = [segment_starts[0] + duration]   \n",
    "        else:\n",
    "            # For segments longer than 1.5 seconds\n",
    "            segment_starts = []\n",
    "            current_start = start_time\n",
    "            while current_start < end_time:\n",
    "                segment_starts.extend([\n",
    "                    # max(round(current_start - (segment_length - overlap_percent * segment_length), 2), 0),\n",
    "                    current_start,\n",
    "                    # min(round(current_start + (segment_length - overlap_percent * segment_length), 2), len(audio) / sr - segment_length)\n",
    "                ])\n",
    "                current_start += segment_length  # Move by segment_length each time\n",
    "            \n",
    "            # Remove duplicates and sort\n",
    "            segment_starts = sorted(set(segment_starts))\n",
    "            segment_ends = [min(start + segment_length, end_time) for start in segment_starts]\n",
    "\n",
    "            # Remove the last segment if it's shorter than segment_length\n",
    "            if segment_ends[-1] - segment_starts[-1] < segment_length:\n",
    "                segment_starts.pop()\n",
    "                segment_ends.pop()\n",
    "        \n",
    "        for seg_start, seg_end in zip(segment_starts, segment_ends):\n",
    "            #print(f\"  Extracting segment from {seg_start} to {seg_end}\")\n",
    "            \n",
    "            # Extract segments\n",
    "            start_idx = int(seg_start * sr)\n",
    "            end_idx = int(seg_end * sr)\n",
    "            original_segment = audio[start_idx:end_idx]\n",
    "            bandpass_segment = filtered_audio[start_idx:end_idx]\n",
    "            denoised_segment = denoised_audio[start_idx:end_idx]\n",
    "            \n",
    "            # Pad segments if necessary\n",
    "            original_segment = pad_audio_with_clipping(original_segment, segment_length, sr, audio, start_idx, end_idx)\n",
    "            bandpass_segment = pad_audio_with_clipping(bandpass_segment, segment_length, sr, filtered_audio, start_idx, end_idx)\n",
    "            denoised_segment = pad_audio_with_clipping(denoised_segment, segment_length, sr, denoised_audio, start_idx, end_idx)\n",
    "            \n",
    "            # Save raw segment\n",
    "            save_segment(original_segment, sr, row['filename'], seg_start, seg_end, species, raw_dir)\n",
    "            save_segment(bandpass_segment, sr, row['filename'], seg_start, seg_end, species, bandpass_dir)\n",
    "            save_segment(denoised_segment, sr, row['filename'], seg_start, seg_end, species, denoised_dir)\n",
    "            \n",
    "            #print(f\"  Saved raw and denoised segments\")\n",
    "\n",
    "def save_segment(segment, sr, original_filename, start, end, species, output_dir):\n",
    "    species_dir = os.path.join(output_dir, species)\n",
    "    os.makedirs(species_dir, exist_ok=True)\n",
    "    \n",
    "    segment_filename = os.path.join(species_dir, f\"{os.path.splitext(original_filename)[0]}_{int(start*1000)}_{int(end*1000)}.wav\")\n",
    "    sf.write(segment_filename, segment, sr)\n",
    "    #print(f\"Saved segment: {segment_filename}\")\n",
    "\n",
    "def process_all_audio_files(df, base_audio_path, output_base_path, fmin=150, fmax=650, segment_length=2.0):\n",
    "    grouped = df.groupby('filename')\n",
    "    \n",
    "    for filename, group_df in grouped:\n",
    "        input_path = os.path.join(base_audio_path, filename)\n",
    "        if os.path.exists(input_path):\n",
    "            #print(f\"Processing file: {input_path}\")\n",
    "            audio, sr = librosa.load(input_path)\n",
    "            process_and_segment_audio(audio, sr, group_df, output_base_path, fmin = fmin, fmax=fmax, segment_length=segment_length)\n",
    "        else:\n",
    "            print(f\"File not found: {input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USER INPUT BEGINS HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/pacific_imperial_pigeon/processed/segmented_audio/raw\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/pacific_imperial_pigeon/processed/segmented_audio/bandpass\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/pacific_imperial_pigeon/processed/segmented_audio/denoised\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/tooth_billed_pigeon/processed/segmented_audio/raw\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/tooth_billed_pigeon/processed/segmented_audio/bandpass\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/tooth_billed_pigeon/processed/segmented_audio/denoised\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/crimson_crowned_fruit_dove/processed/segmented_audio/raw\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/crimson_crowned_fruit_dove/processed/segmented_audio/bandpass\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/crimson_crowned_fruit_dove/processed/segmented_audio/denoised\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/white_throated_pigeon/processed/segmented_audio/raw\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/white_throated_pigeon/processed/segmented_audio/bandpass\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/white_throated_pigeon/processed/segmented_audio/denoised\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/honeyeater/processed/segmented_audio/raw\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/honeyeater/processed/segmented_audio/bandpass\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/honeyeater/processed/segmented_audio/denoised\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/samoan_starling/processed/segmented_audio/raw\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/samoan_starling/processed/segmented_audio/bandpass\n",
      "Created directory: /home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data/samoan_starling/processed/segmented_audio/denoised\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the raw audio files\n",
    "#base_dir = \"/home/leah_colossal_com/et_dataset/us_bird_train\"\n",
    "base_dir = \"/home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data\"\n",
    "\n",
    "folders = [name for name in os.listdir(base_dir) \n",
    "    if os.path.isdir(os.path.join(base_dir, name)) and name != 'model']\n",
    "#folders = ['white_throated_sparrow','northern_cardinal','carolina_wren',\n",
    "#                                'eastern_towhee','kentucky_warbler']\n",
    "\n",
    "\n",
    "# define the bandpass filters that should be used for each species, this helps with denoising, \n",
    "# for this example, we just set to max limits and it still works, but if you have many overlapping calls, it is helpful to bandpass \n",
    "bp_filters = {'white_throated_sparrow':[2500,5000],\n",
    "              'northern_cardinal':[1100,5000],\n",
    "              'carolina_wren':[1400,5000],\n",
    "              'eastern_towhee':[2500,6500],\n",
    "              'kentucky_warbler':[2000,5500],\n",
    "              'tooth_billed_pigeon':[100,700],\n",
    "              'pacific_imperial_pigeon':[100,700],\n",
    "              'crimson_crowned_fruit_dove':[200,900],\n",
    "              'samoan_starling':[1300,4200],  \n",
    "              'honeyeater':[900,4000],\n",
    "              'white_throated_pigeon':[100,700]\n",
    "             }\n",
    "\n",
    "\n",
    "#bp_filter = [2500,6500] # eastern towhee\n",
    "#bp_filter = [100,700] # tooth-billed pigeon\n",
    "#bp_filter = [1,8000] # no filter\n",
    "\n",
    "for folder_idx,folder in enumerate(folders):\n",
    "    \n",
    "    # define the files needed\n",
    "    species_label = folders[folder_idx]\n",
    "    raw_dir = os.path.join(base_dir,folder,'raw')\n",
    "    processed_dir = os.path.join(base_dir,folder,'processed')\n",
    "    \n",
    "    # get bandpass limits for this particular species\n",
    "    bandpass_lower_limit = bp_filters[folder][0]\n",
    "    bandpass_upper_limit = bp_filters[folder][1]\n",
    "    #bandpass_lower_limit = bp_filter[0]\n",
    "    #bandpass_upper_limit = bp_filter[1]\n",
    "  \n",
    "    \n",
    "    # Preprocess audio for all species\n",
    "    csv_file_path = find_csv_file(raw_dir)\n",
    "    #csv_file_path = os.path.join(raw_dir,'event_detections2.csv'\n",
    "    create_directory_structure(processed_dir)\n",
    "    segmented_output_path = os.path.join(processed_dir,'segmented_audio')\n",
    "\n",
    "    if csv_file_path:\n",
    "        metadata_df = load_metadata(csv_file_path)\n",
    "        if metadata_df is not None:\n",
    "            process_all_audio_files(metadata_df, raw_dir, processed_dir,\n",
    "                            fmin = bandpass_lower_limit,fmax = bandpass_upper_limit)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the end of the preprocessing code\n",
    "# the rest of this code are supporting visualization and listening functions, if you'd like to check on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def display_audio_and_waveforms(audio_list, sr, show_audio=True):\n",
    "    \"\"\"\n",
    "    Display audio files and their waveforms.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_list (list): List of tuples (audio_array, label).\n",
    "    sr (int): Sample rate.\n",
    "    show_audio (bool): Whether to display the audio players.\n",
    "    \"\"\"\n",
    "    # Plot waveforms\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    colors = ['crimson', 'deepskyblue' ,'r', 'c', 'm', 'y', 'k']\n",
    "    \n",
    "    for i, (audio_data, label) in enumerate(audio_list):\n",
    "        if show_audio:\n",
    "            display(Audio(audio_data, rate=sr, autoplay=False))\n",
    "        \n",
    "        color = colors[i % len(colors)]\n",
    "        alpha = 0.75 if i < len(audio_list) - 1 else 0.95\n",
    "        librosa.display.waveshow(audio_data, sr=sr, alpha=alpha, color=color, label=label)\n",
    "    \n",
    "    plt.title('Waveforms')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def display_spectrogram(audio_list,sr):\n",
    "    \"\"\"\n",
    "    Display the spectrogram of the audio files.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_list (list): List of tuples (audio_array, label).\n",
    "    sr (int): Sample rate.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    threshold = -40\n",
    "    for i, (audio_data, label) in enumerate(audio_list):\n",
    "        plt.subplot(1,len(audio_list), i + 1)\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)\n",
    "        D[D<threshold] = threshold\n",
    "        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log', label=label)\n",
    "        plt.ylim([0, 1024])\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(f'Spectrogram - {label}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_mel_spectrogram(audio_list, sr):\n",
    "    \"\"\"\n",
    "    Display the mel spectrogram of the audio files side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_list (list): List of tuples (audio_array, label).\n",
    "    sr (int): Sample rate.\n",
    "    \"\"\"\n",
    "    num_audios = len(audio_list)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    threshold = -40\n",
    "    for i, (audio_data, label) in enumerate(audio_list):\n",
    "        plt.subplot(1, num_audios, i + 1)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_mels=128)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        log_mel_spectrogram[log_mel_spectrogram<threshold] = threshold\n",
    "        librosa.display.specshow(log_mel_spectrogram, sr=sr, x_axis='time', y_axis='mel')\n",
    "        plt.ylim([0, 1024])\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title(f'Mel Spectrogram - {label}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_frequency_domain(audio_list, sr):\n",
    "    \"\"\"\n",
    "    Display the frequency domain of the audio files.\n",
    "    \n",
    "    Parameters:\n",
    "    audio_list (list): List of tuples (audio_array, label).\n",
    "    sr (int): Sample rate.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i, (audio_data, label) in enumerate(audio_list):\n",
    "        frequencies = np.fft.rfftfreq(len(audio_data), 1/sr)\n",
    "        magnitude = np.abs(np.fft.rfft(audio_data))\n",
    "        \n",
    "        plt.subplot(len(audio_list), 1, i + 1)\n",
    "        plt.plot(frequencies, magnitude)\n",
    "        plt.xlim(0, 1024)\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.title(f'Magnitude vs Frequency ({label} Audio)')\n",
    "        plt.grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for file in os.listdir(os.path.join(processed_dir, 'segmented_audio', 'raw', species_label))[:5]:\n",
    "    #print(file)\n",
    "    \n",
    "    # Load raw (filtered) audio\n",
    "    y_raw, sr = librosa.load(os.path.join(processed_dir, 'segmented_audio', 'raw', species_label, file), sr=None)\n",
    "    \n",
    "    # Load denoised audio\n",
    "    y_denoised, _ = librosa.load(os.path.join(processed_dir, 'segmented_audio', 'denoised', species_label, file), sr=None)\n",
    "    \n",
    "    audio_list = [\n",
    "        (y_raw, 'Filtered'),\n",
    "        (y_denoised, 'Denoised'),\n",
    "    ]\n",
    "    \n",
    "    display_audio_and_waveforms(audio_list, sr, show_audio=True)\n",
    "    display_spectrogram(audio_list, sr)\n",
    "    display_mel_spectrogram(audio_list, sr)\n",
    "    display_frequency_domain(audio_list, sr)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-audio-audio",
   "name": "workbench-notebooks.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
  },
  "kernelspec": {
   "display_name": "audio (Local)",
   "language": "python",
   "name": "conda-env-audio-audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

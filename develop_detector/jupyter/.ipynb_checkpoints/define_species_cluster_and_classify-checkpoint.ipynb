{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3341370d-ffce-49bd-96e3-7ee674c2a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: vector of all embeddings with corresponding species label, label for species of interest\n",
    "# outputs: embedding vector of species of interest, threshold for classification\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib  # For saving and loading the PCA model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "101e5368-3d17-4f2c-9ec5-3c40ffc65f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MakePrediction:\n",
    "    def __init__(self, model_folder=\"./model\"):\n",
    "        \"\"\"Initialize with PCA configuration and model folder path.\"\"\"\n",
    "        self.model_folder = model_folder\n",
    "        os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "    def fit_pca(self, embeddings, n_components=142):\n",
    "        \"\"\"Fit PCA on embeddings and save the model.\"\"\"\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        reduced_embeddings = self.pca.fit_transform(embeddings)\n",
    "        joblib.dump(self.pca, os.path.join(self.model_folder, \"pca_model.joblib\"))\n",
    "        return reduced_embeddings\n",
    "\n",
    "    def load_pca(self):\n",
    "        \"\"\"Load the PCA model from the model folder.\"\"\"\n",
    "        pca_path = os.path.join(self.model_folder, \"pca_model.joblib\")\n",
    "        if os.path.exists(pca_path):\n",
    "            self.pca = joblib.load(pca_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"PCA model not found. Please run fit_pca first.\")\n",
    "\n",
    "    def transform_pca(self, embeddings):\n",
    "        \"\"\"Normalize embeddings and transform using PCA.\"\"\"\n",
    "        if not hasattr(self.pca, \"components_\"):\n",
    "            self.load_pca()\n",
    "        normalized_embeddings = normalize(embeddings, axis=1, norm=\"l2\")\n",
    "        return self.pca.transform(normalized_embeddings)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_prototype(embeddings):\n",
    "        \"\"\"Calculate the prototype vector for a single species.\"\"\"\n",
    "        return np.median(embeddings, axis=0, keepdims=True)\n",
    "\n",
    "    def save_species_prototype(self, species_prototype):\n",
    "        \"\"\"Save the species of interest prototype.\"\"\"\n",
    "        np.save(os.path.join(self.model_folder, \"species_prototype.npy\"), species_prototype)\n",
    "\n",
    "    def load_species_prototype(self):\n",
    "        \"\"\"Load the species of interest prototype.\"\"\"\n",
    "        prototype_path = os.path.join(self.model_folder, \"species_prototype.npy\")\n",
    "        if os.path.exists(prototype_path):\n",
    "            return np.load(prototype_path)\n",
    "        raise FileNotFoundError(\"Species prototype not found. Please run make_classifier first.\")\n",
    "\n",
    "    def determine_threshold(self, embeddings, labels, soi_label, min_recall):\n",
    "        \"\"\"Determine cosine similarity threshold to achieve minimum recall.\"\"\"\n",
    "        reduced_embeddings = self.transform_pca(embeddings)\n",
    "\n",
    "        # Extract SOI embeddings and calculate its prototype\n",
    "        soi_mask = np.array(labels) == soi_label\n",
    "        other_mask = ~soi_mask\n",
    "        soi_embeddings = reduced_embeddings[soi_mask]\n",
    "        other_embeddings = reduced_embeddings[other_mask]\n",
    "\n",
    "        # Calculate the SOI prototype (median embedding of the species of interest)\n",
    "        species_prototype = self.calculate_prototype(soi_embeddings)\n",
    "        self.save_species_prototype(species_prototype)\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        soi_similarities = cosine_similarity(soi_embeddings, species_prototype).flatten()\n",
    "        other_similarities = cosine_similarity(other_embeddings, species_prototype).flatten()\n",
    "\n",
    "        # Determine the minimum threshold for recall\n",
    "        min_required = int(np.ceil(min_recall * len(soi_similarities)))\n",
    "        threshold = np.sort(soi_similarities)[::-1][min_required - 1]\n",
    "\n",
    "        self.threshold = threshold\n",
    "        np.save(os.path.join(self.model_folder, \"threshold.npy\"), threshold)\n",
    "        return threshold\n",
    "\n",
    "    def make_classifier(self, embeddings, labels, soi_label, min_recall, n_components=142):\n",
    "        \"\"\"Create classifier with PCA, species prototype, and threshold.\"\"\"\n",
    "        # Fit PCA with the embeddings\n",
    "        self.fit_pca(embeddings, n_components=min(n_components, embeddings.shape[0]))\n",
    "\n",
    "        # Determine the threshold and save the SOI prototype\n",
    "        self.soi_label = soi_label  # Store species of interest label\n",
    "        self.threshold = self.determine_threshold(embeddings, labels, soi_label, min_recall)\n",
    "\n",
    "        # Save the SOI label\n",
    "        soi_label_path = os.path.join(self.model_folder, \"soi_label.txt\")\n",
    "        with open(soi_label_path, \"w\") as f:\n",
    "            f.write(soi_label)\n",
    "\n",
    "    def classify(self, unknown_embeddings, baseline_probability=1.0 / 6):\n",
    "        \"\"\"\n",
    "        Classify unknown embeddings and provide predictions with confidence scores.\n",
    "\n",
    "        Args:\n",
    "            unknown_embeddings (np.ndarray): Embeddings to classify.\n",
    "            baseline_probability (float): Baseline probability for relative confidence (default 1/6).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Predictions, confidence scores, and relative confidence values.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"species_prototype\"):\n",
    "            self.load_classifier()\n",
    "    \n",
    "        # Transform unknown embeddings using the trained PCA\n",
    "        reduced_embeddings = self.transform_pca(unknown_embeddings)\n",
    "\n",
    "        # Load the species prototype\n",
    "        species_prototype = self.load_species_prototype()\n",
    "\n",
    "        # Compute cosine similarity between unknown embeddings and the species prototype\n",
    "        similarities = cosine_similarity(reduced_embeddings, species_prototype).flatten()\n",
    "\n",
    "        # Classify based on the threshold\n",
    "        predictions = [\n",
    "            \"species_of_interest\" if similarity >= self.threshold else \"not_species_of_interest\"\n",
    "            for similarity in similarities\n",
    "        ]\n",
    "\n",
    "        # Calculate confidence scores\n",
    "        confidence_scores = similarities  # Cosine similarity serves as the confidence score\n",
    "\n",
    "        # Calculate relative confidence scores\n",
    "        relative_confidences = [\n",
    "            ((score - baseline_probability) / baseline_probability) * 100\n",
    "            for score in confidence_scores\n",
    "        ]\n",
    "\n",
    "        # Create a DataFrame to store results\n",
    "        results_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Predicted Label\": predictions,\n",
    "                \"Confidence Score\": confidence_scores,\n",
    "                \"Relative Confidence (%)\": [round(rc, 2) for rc in relative_confidences],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return results_df\n",
    "    \n",
    "    def load_classifier(self):\n",
    "        \"\"\"Load PCA, species prototype, threshold, and SOI label from the model folder.\"\"\"\n",
    "        # Load PCA model\n",
    "        self.load_pca()\n",
    "\n",
    "        # Load species prototype\n",
    "        species_prototype_path = os.path.join(self.model_folder, \"species_prototype.npy\")\n",
    "        if os.path.exists(species_prototype_path):\n",
    "            self.species_prototype = np.load(species_prototype_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Species prototype not found. Ensure it was saved during make_classifier.\")\n",
    "\n",
    "        # Load threshold\n",
    "        threshold_path = os.path.join(self.model_folder, \"threshold.npy\")\n",
    "        if os.path.exists(threshold_path):\n",
    "            self.threshold = float(np.load(threshold_path))\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Threshold not found. Ensure it was saved during make_classifier.\")\n",
    "\n",
    "        # Load species of interest label\n",
    "        soi_label_path = os.path.join(self.model_folder, \"soi_label.txt\")\n",
    "        if os.path.exists(soi_label_path):\n",
    "            with open(soi_label_path, \"r\") as f:\n",
    "                self.soi_label = f.read().strip()\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Species of interest label not found. Ensure it was saved during make_classifier.\")\n",
    "\n",
    "def load_embeddings_labels(embeddings_file,labels_file):\n",
    "    \n",
    "    loaded = np.load(embeddings_file)\n",
    "    embeddings = [loaded[key] for key in loaded]\n",
    "    #embeddings = np.load(embeddings_file)\n",
    "    \n",
    "    labels = np.load(labels_file)\n",
    "    return embeddings, labels\n",
    "\n",
    "def load_embeddings_labels_folders(base_dir,folders,\n",
    "                    label_name = 'labels.npy', embeddings_name = 'embeddings.npz'):\n",
    "    \n",
    "    embeddings_all = []\n",
    "    labels_all = []\n",
    "    for folder in folders:\n",
    "        processed_dir = os.path.join(base_dir,folder,\"processed\")\n",
    "        labels_file = os.path.join(processed_dir,label_name)\n",
    "        embeddings_file = os.path.join(processed_dir,embeddings_name)\n",
    "\n",
    "        embeddings, labels = load_embeddings_labels(embeddings_file,labels_file)\n",
    "        embeddings_all += embeddings\n",
    "        labels_all = np.concatenate((labels_all, labels))\n",
    "        \n",
    "    embeddings_npy = np.vstack(embeddings_all)\n",
    "    labels_npy = np.vstack(labels_all)\n",
    "    \n",
    "    return embeddings_npy, labels_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5807986b-56b4-4b8a-9833-9ed011273836",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Develop the classifier based on the embeddings and labels of the training dataset\n",
    "\n",
    "# define training folder\n",
    "#base_dir = \"/home/leah_colossal_com/us_bird_train\"\n",
    "base_dir = \"/home/leah_colossal_com/tbp_dataset/balanced_dataset/train_data\"\n",
    "folders = [name for name in os.listdir(base_dir) \n",
    "    if os.path.isdir(os.path.join(base_dir, name)) and name != 'model']\n",
    "#folders = ['white_throated_sparrow','northern_cardinal','carolina_wren','eastern_towhee','kentucky_warbler']\n",
    "\n",
    "model_folder = os.path.join(base_dir,\"model\") # this is where the saved classifier will go\n",
    "      \n",
    "# Make classifier based on training data\n",
    "min_recall = 0.95 # Threshold of minium recall allowed\n",
    "embeddings,labels = load_embeddings_labels_folders(base_dir,folders)\n",
    "\n",
    "# make the predictor\n",
    "predictor = MakePrediction(model_folder = model_folder)\n",
    "#soi_label = 'eastern_towhee' # this is the label for the species of interest\n",
    "soi_label = 'tooth_billed_pigeon' # this is the label for the species of interest\n",
    "\n",
    "# ^ the classifier is a binary classifier of species of interest vs not. \n",
    "\n",
    "predictor.make_classifier(embeddings,labels,soi_label,min_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f69000-c0d8-454b-b1ae-66e3b0d87f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classify, the code below uses the trained classifier to classify new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d0e49ae3-4c21-473c-9f50-8c31895f0fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the test set on this developed model\n",
    "\n",
    "# load test embeddings\n",
    "#test_base_dir = \"/home/leah_colossal_com/us_bird_test\"\n",
    "test_base_dir = \"/home/leah_colossal_com/tbp_dataset/balanced_dataset/test_data\"\n",
    "\n",
    "#folders = ['white_throated_sparrow','northern_cardinal','carolina_wren','eastern_towhee','kentucky_warbler']\n",
    "embeddings_test,labels = load_embeddings_labels_folders(test_base_dir,folders)\n",
    "\n",
    "results_df = predictor.classify(embeddings_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7644c50f-bf63-4cfe-bfee-b92e0e964f12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the test embeddings using the load_classifier function\n",
    "# If you're re-using an classifier you've already developed, you can run the first two cells, then jump to here! \n",
    "\n",
    "test_base_dir = \"/home/leah_colossal_com/us_bird_test\"\n",
    "folders = ['white_throated_sparrow','northern_cardinal','carolina_wren','eastern_towhee','kentucky_warbler']\n",
    "model_folder = \"/home/leah_colossal_com/us_bird_train/model\" # This is from train, because that's where we made the model\n",
    "      \n",
    "predictor = MakePrediction(model_folder = model_folder)\n",
    "predictor.load_classifier()\n",
    "\n",
    "embeddings_test,labels = load_embeddings_labels_folders(test_base_dir,folders)\n",
    "results_df = predictor.classify(embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b5cc1e89-7b9f-41a7-afa4-06b9154291b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for species of interest 'tooth_billed_pigeon': 98.78%\n",
      "Recall for species of interest 'tooth_billed_pigeon': 100.00%\n",
      "F1-score for species of interest 'tooth_billed_pigeon': 92.31%\n"
     ]
    }
   ],
   "source": [
    "# Generate metrics for the results. Giving accuracy, recall and F1\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Extract the true labels for the species of interest\n",
    "true_positive_mask = np.array(labels) == predictor.soi_label\n",
    "\n",
    "# Get the predicted labels from the results DataFrame\n",
    "predicted_labels = np.array(results_df['Predicted Label'])\n",
    "\n",
    "# Binary true labels for species of interest (1 for SOI, 0 for others)\n",
    "binary_true_labels = np.where(true_positive_mask, 1, 0)\n",
    "\n",
    "# Binary predicted labels for species of interest (1 for 'species_of_interest', 0 for others)\n",
    "binary_predicted_labels = np.where(predicted_labels == 'species_of_interest', 1, 0)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_soi = sum(binary_true_labels == binary_predicted_labels) / len(binary_true_labels) if len(binary_true_labels) > 0 else 0\n",
    "recall_soi = recall_score(binary_true_labels, binary_predicted_labels, zero_division=0)\n",
    "precision_soi = precision_score(binary_true_labels, binary_predicted_labels, zero_division=0)\n",
    "f1_soi = f1_score(binary_true_labels, binary_predicted_labels, zero_division=0)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy for species of interest '{predictor.soi_label}': {accuracy_soi:.2%}\")\n",
    "print(f\"Recall for species of interest '{predictor.soi_label}': {recall_soi:.2%}\")\n",
    "print(f\"F1-score for species of interest '{predictor.soi_label}': {f1_soi:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-audio-audio",
   "name": "workbench-notebooks.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
  },
  "kernelspec": {
   "display_name": "audio (Local)",
   "language": "python",
   "name": "conda-env-audio-audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

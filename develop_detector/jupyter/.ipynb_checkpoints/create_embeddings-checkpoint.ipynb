{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b239544-aa9b-4274-b5e2-0b9eb5b0f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: path to preprocessed audio files, labels, from 'preprocess.ipynb'\n",
    "# output: saved embeddings of all clips, with their labels\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow_hub as hub \n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c888d293-fc2e-4f00-89bb-249fa3c1947d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the embedding generator class \n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_url='https://www.kaggle.com/models/google/bird-vocalization-classifier/TensorFlow2/bird-vocalization-classifier/8', sr=32000, target_samples=160000):\n",
    "        \"\"\" \n",
    "        Initialize the EmbeddingGenerator with the Bird Vocalization Classifier model.\n",
    "\n",
    "        Args:                                                                                                                           \n",
    "            model_url (str): URL to load the Bird Vocalization Classifier model.\n",
    "            sr (int): Sample rate for the audio segments (default is 32000).\n",
    "            target_samples (int): Target number of samples for each segment (default is 160000).\n",
    "        \"\"\"\n",
    "        self.model = hub.load(model_url)\n",
    "        self.sr = sr\n",
    "        self.target_samples = target_samples\n",
    "\n",
    "    def preprocess_segment(self, segment):\n",
    "        \"\"\" \n",
    "        Preprocesses a single audio segment by padding or truncating to the target sample length.\n",
    "\n",
    "        Args:\n",
    "            segment (np.array): The audio segment to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Padded or truncated audio segment ready for embedding.\n",
    "        \"\"\"\n",
    "        current_samples = len(segment)\n",
    "            \n",
    "        if current_samples < self.target_samples:\n",
    "            # Calculate padding lengths and pad with zeros\n",
    "            pad_length = self.target_samples - current_samples\n",
    "            pad_left = pad_length // 2\n",
    "            pad_right = pad_length - pad_left\n",
    "            padded_segment = np.pad(segment, (pad_left, pad_right), 'constant')\n",
    "            return padded_segment\n",
    "        else:\n",
    "            # Truncate if longer than target duration\n",
    "            return segment[:self.target_samples]\n",
    "        \n",
    "    def generate_embeddings(self, segments):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a list of audio segments using the Bird Vocalization Classifier model.\n",
    "\n",
    "        Args:\n",
    "            segments (list of np.array): List of preprocessed audio segments.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Array of embeddings for each segment.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for segment in segments:\n",
    "            # Preprocess each segment to match the model's target sample length\n",
    "            processed_segment = self.preprocess_segment(segment)\n",
    "            \n",
    "            # Reshape to fit the model's expected input dimensions\n",
    "            audio_input = processed_segment[np.newaxis, :]  # Shape (1, 160000)\n",
    "            \n",
    "            # Generate embeddings using the model\n",
    "            result = self.model.infer_tf(audio_input)\n",
    "            embedding = result['embedding']\n",
    "            \n",
    "            # Flatten and store the embedding\n",
    "            embeddings.append(embedding.numpy().flatten())\n",
    "\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "# define the segment loader: \n",
    "\n",
    "\n",
    "def load_denoised_segments(output_base_path):\n",
    "    \"\"\"\n",
    "    Load the denoised audio segments from the saved directory.\n",
    "\n",
    "    Args:\n",
    "    - output_base_path (str): Base path for output directories where the function saved the segments.\n",
    "\n",
    "    Returns:\n",
    "    - denoised_segments (list): A list of numpy arrays representing the denoised audio segments.\n",
    "    \"\"\"\n",
    "    denoised_segments = []\n",
    "    \n",
    "    denoised_dir = os.path.join(output_base_path, 'segmented_audio', 'denoised')\n",
    "\n",
    "    # Traverse the species directories within the denoised directory\n",
    "    for species_dir in os.listdir(denoised_dir):\n",
    "        species_path = os.path.join(denoised_dir, species_dir)\n",
    "\n",
    "        if os.path.isdir(species_path):  # Ensure it's a directory\n",
    "            # Load all .wav files in the species directory\n",
    "            for file in os.listdir(species_path):\n",
    "                if file.endswith('.wav'):  # Only process .wav files\n",
    "                    file_path = os.path.join(species_path, file)\n",
    "                    # Load the audio file\n",
    "                    try:\n",
    "                        audio, sr = librosa.load(file_path, sr=None)  # Load with original sampling rate\n",
    "                        denoised_segments.append(audio)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading file {file_path}: {e}\")\n",
    "    \n",
    "    return denoised_segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53931f86-0854-40bf-bb85-8e16c52e18c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/leah_colossal_com/tbp_dataset/balanced_dataset/test_data/model/processed/segmented_audio/denoised'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m embeddings_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(processed_dir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)               \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# load the preprocessed data                \u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m denoised_segments \u001b[38;5;241m=\u001b[39m \u001b[43mload_denoised_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(denoised_segments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m denoised segments.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate and save embeddings for the denoised segments\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 86\u001b[0m, in \u001b[0;36mload_denoised_segments\u001b[0;34m(output_base_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m denoised_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_base_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegmented_audio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdenoised\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Traverse the species directories within the denoised directory\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m species_dir \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(denoised_dir):\n\u001b[1;32m     87\u001b[0m     species_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(denoised_dir, species_dir)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(species_path):  \u001b[38;5;66;03m# Ensure it's a directory\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# Load all .wav files in the species directory\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/leah_colossal_com/tbp_dataset/balanced_dataset/test_data/model/processed/segmented_audio/denoised'"
     ]
    }
   ],
   "source": [
    "# define where the data is, and the folders to read\n",
    "base_dir = \"/home/leah_colossal_com/tbp_dataset/balanced_dataset/test_data\"\n",
    "\n",
    "folders = [name for name in os.listdir(base_dir) \n",
    "    if os.path.isdir(os.path.join(base_dir, name)) and name != 'model']\n",
    "\n",
    "#folders = ['white_throated_sparrow','northern_cardinal',\n",
    "#                'carolina_wren','eastern_towhee', 'kentucky_warbler']\n",
    "\n",
    "\n",
    "# Make embeddings for each folder\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "for folder_idx,folder in enumerate(folders):\n",
    "\n",
    "    # define files and folders\n",
    "    processed_dir = os.path.join(base_dir,folder,'processed')\n",
    "    labels_file = os.path.join(processed_dir,\"labels.npy\")\n",
    "    embeddings_file = os.path.join(processed_dir,\"embeddings.npz\")               \n",
    "    \n",
    "    # load the preprocessed data                \n",
    "    denoised_segments = load_denoised_segments(processed_dir)\n",
    "    print(f\"Loaded {len(denoised_segments)} denoised segments.\")\n",
    "    \n",
    "    # Generate and save embeddings for the denoised segments\n",
    "    embeddings = embedding_generator.generate_embeddings(denoised_segments)\n",
    "    np.savez_compressed(embeddings_file, *embeddings)\n",
    "\n",
    "    # Generate and save labels for these embeddings\n",
    "    labels = [folder]*len(embeddings)\n",
    "    np.save(labels_file, labels)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-audio-audio",
   "name": "workbench-notebooks.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m122"
  },
  "kernelspec": {
   "display_name": "audio (Local)",
   "language": "python",
   "name": "conda-env-audio-audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
